{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "daf688e5-c4d4-416d-92b5-0b2ed851f129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5d0c9e8d22422eb5e41a419e2253a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup model, data, helpers, adapters from SFT\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import os, json, gzip, hashlib, random, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "@dataclass\n",
    "class GenCfg:\n",
    "    base_model: str = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "    adapter_dir: str = \"/workspace/output/cai_sft_stage1/adapters\"\n",
    "    cache_dir: str = \"/workspace/hf-cache\"\n",
    "    redteam_path: str = \"/workspace/red_team_prompts.jsonl\"  # one JSONL per line with {\"prompt\": \"...\"}\n",
    "    out_dir: str = \"/workspace/output/redteam_pairs\"\n",
    "    out_file: str = \"pairs.jsonl.gz\"\n",
    "    system_msg: str | None = None\n",
    "    max_new_tokens: int = 384\n",
    "    seq_len: int = 1024\n",
    "    t1: float = 0.7\n",
    "    t2: float = 1.0\n",
    "    seed: int = 1234\n",
    "\n",
    "cfg = GenCfg()\n",
    "os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "\n",
    "bnb = BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_quant_type=\"nf4\",bnb_4bit_use_double_quant=True,bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(cfg.base_model, use_fast=True, cache_dir=cfg.cache_dir, local_files_only=True)\n",
    "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"left\"\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.base_model,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    cache_dir=cfg.cache_dir,\n",
    "    local_files_only=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, cfg.adapter_dir)\n",
    "model.eval()\n",
    "model.config.use_cache = True\n",
    "\n",
    "def sample_id(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def append_jsonl_gz(path: str, rows: list[dict]):\n",
    "    tmp = path + \".tmp\"\n",
    "    mode = \"ab\" if os.path.exists(path) else \"wb\"\n",
    "    with gzip.open(tmp, mode) as f:\n",
    "        for r in rows:\n",
    "            f.write((json.dumps(r, ensure_ascii=False) + \"\\n\").encode(\"utf-8\"))\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def load_done_ids(path: str) -> set[str]:\n",
    "    ids = set()\n",
    "    if not os.path.exists(path): return ids\n",
    "    with gzip.open(path, \"rb\") as f:\n",
    "        for b in f:\n",
    "            try:\n",
    "                ids.add(json.loads(b.decode(\"utf-8\")).get(\"id\"))\n",
    "            except:\n",
    "                pass\n",
    "    return ids\n",
    "\n",
    "def conv_from_prompt(p: str) -> str:\n",
    "    msgs = []\n",
    "    if cfg.system_msg: msgs.append({\"role\":\"system\",\"content\":cfg.system_msg})\n",
    "    msgs.append({\"role\":\"user\",\"content\":p})\n",
    "    return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "def prefill_from_prompt(p: str) -> str:\n",
    "    msgs = []\n",
    "    if cfg.system_msg: msgs.append({\"role\":\"system\",\"content\":cfg.system_msg})\n",
    "    msgs.append({\"role\":\"user\",\"content\":p})\n",
    "    return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1db4b016-6055-42bc-a756-6e17654e240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active: default\n",
      "adapters: ['default']\n",
      "LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='meta-llama/Llama-3.3-70B-Instruct', revision=None, inference_mode=True, r=8, target_modules={'q_proj', 'up_proj', 'k_proj', 'o_proj', 'down_proj', 'v_proj', 'gate_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)\n",
      "lora_params: 103546880\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "assert isinstance(model, PeftModel)\n",
    "print(\"active:\", model.active_adapter)\n",
    "print(\"adapters:\", list(model.peft_config.keys()))\n",
    "print(model.peft_config[\"default\"])\n",
    "\n",
    "n_lora = sum(p.numel() for n,p in model.named_parameters() if \"lora_\" in n)\n",
    "print(\"lora_params:\", n_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ae54558-5a6e-438a-bd3a-e1aa4ef88395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b4560becd84c68bf591e71e2f89914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48a75a25e5740b39080082500899c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/37163 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "prompts = load_dataset(\"json\", data_files=\"red_team_prompts.jsonl\", split=\"train\")\n",
    "prompts = prompts.select_columns([\"prompt\"])\n",
    "prompts = prompts.filter(lambda x: isinstance(x[\"prompt\"], str) and len(x[\"prompt\"].strip())>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62a3aac5-d780-4fb9-b38c-2e5289e69c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c1b852f8c14802acac18ac6c8a19c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Run 20250911-035447 — new pairs:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run 20250911-035447] wrote 10000 new pairs → /workspace/cai_runs/20250911-035447/pairs.jsonl.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969ff9a6b7ff4e69929f111320b7e277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs on disk: 10052\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, gzip, hashlib, random\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ROOT = \"/workspace/cai_runs\"\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "runid_path = os.path.join(ROOT, \"latest.runid\")\n",
    "if os.path.exists(runid_path):\n",
    "    RUN_ID = open(runid_path, \"r\").read().strip()\n",
    "else:\n",
    "    RUN_ID = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    with open(runid_path, \"w\") as f: f.write(RUN_ID)\n",
    "BASE_DIR = f\"{ROOT}/{RUN_ID}\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "GEN_JSONL = os.path.join(BASE_DIR, \"pairs.jsonl.gz\")\n",
    "\n",
    "def sample_id(prompt: str) -> str:\n",
    "    return hashlib.sha1(prompt.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def append_jsonl_gz(path: str, rows: list[dict]):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with gzip.open(path, \"ab\") as f:\n",
    "        for r in rows:\n",
    "            f.write((json.dumps(r, ensure_ascii=False) + \"\\n\").encode(\"utf-8\"))\n",
    "\n",
    "def build_chat(u: str) -> str:\n",
    "    msgs = [{\"role\":\"user\",\"content\":u}] if not getattr(cfg, \"system_msg\", None) else [{\"role\":\"system\",\"content\":cfg.system_msg},{\"role\":\"user\",\"content\":u}]\n",
    "    return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "tok.padding_side = \"left\"\n",
    "tok.pad_token = tok.eos_token\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tok, return_full_text=False)\n",
    "\n",
    "done_ids = set()\n",
    "if os.path.exists(GEN_JSONL):\n",
    "    ds_done = load_dataset(\"json\", data_files=GEN_JSONL, split=\"train\")\n",
    "    if \"pid\" in ds_done.column_names:\n",
    "        done_ids = set(ds_done[\"pid\"])\n",
    "\n",
    "MAX_PAIRS = 10000\n",
    "BATCH = 16\n",
    "T1, T2 = 0.7, 1.0\n",
    "\n",
    "out_count = 0\n",
    "texts, metas = [], []\n",
    "pbar = tqdm(total=MAX_PAIRS, desc=f\"Run {RUN_ID} — new pairs\", dynamic_ncols=True)\n",
    "\n",
    "try:\n",
    "    for ex in prompts:\n",
    "        if out_count >= MAX_PAIRS:\n",
    "            break\n",
    "        pid = sample_id(ex[\"prompt\"])\n",
    "        if pid in done_ids:\n",
    "            continue\n",
    "        text = build_chat(ex[\"prompt\"])\n",
    "        texts.append(text)\n",
    "        metas.append({\"pid\": pid, \"prompt\": ex[\"prompt\"]})\n",
    "        need = MAX_PAIRS - out_count\n",
    "        eot_id = tok.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        if len(texts) == min(BATCH, need):\n",
    "            outs1 = pipe(\n",
    "                texts,\n",
    "                max_new_tokens=cfg.max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=T1,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tok.eos_token_id,\n",
    "                eos_token_id=[tok.eos_token_id, eot_id],\n",
    "                batch_size=len(texts),\n",
    "            )\n",
    "            outs2 = pipe(\n",
    "                texts,\n",
    "                max_new_tokens=cfg.max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=T2,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tok.eos_token_id,\n",
    "                eos_token_id=[tok.eos_token_id, eot_id],\n",
    "                batch_size=len(texts),\n",
    "            )\n",
    "            rows = []\n",
    "            for m, o1, o2 in zip(metas, outs1, outs2):\n",
    "                a, b = o1[0][\"generated_text\"].strip(), o2[0][\"generated_text\"].strip()\n",
    "                if random.random() < 0.5:\n",
    "                    rows.append({\n",
    "                        \"pid\": m[\"pid\"],\n",
    "                        \"prompt\": m[\"prompt\"],\n",
    "                        \"a_text\": a,\n",
    "                        \"b_text\": b,\n",
    "                        \"a_meta\": {\"temperature\": T1, \"top_p\": 0.9},\n",
    "                        \"b_meta\": {\"temperature\": T2, \"top_p\": 0.9},\n",
    "                        \"order\": \"A=t1,B=t2\",\n",
    "                        \"model\": cfg.base_model,\n",
    "                    })\n",
    "                else:\n",
    "                    rows.append({\n",
    "                        \"pid\": m[\"pid\"],\n",
    "                        \"prompt\": m[\"prompt\"],\n",
    "                        \"a_text\": b,\n",
    "                        \"b_text\": a,\n",
    "                        \"a_meta\": {\"temperature\": T2, \"top_p\": 0.9,},\n",
    "                        \"b_meta\": {\"temperature\": T1, \"top_p\": 0.9},\n",
    "                        \"order\": \"A=t2,B=t1\",\n",
    "                        \"model\": cfg.base_model,\n",
    "                    })\n",
    "            append_jsonl_gz(GEN_JSONL, rows)\n",
    "            out_count += len(rows)\n",
    "            pbar.update(len(rows))\n",
    "            texts, metas = [], []\n",
    "    if texts and out_count < MAX_PAIRS:\n",
    "        need = min(MAX_PAIRS - out_count, len(texts))\n",
    "        outs1 = pipe(\n",
    "            texts[:need],\n",
    "            max_new_tokens=cfg.max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=T1,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "            eos_token_id=[tok.eos_token_id, eot_id],\n",
    "            batch_size=need,\n",
    "        )\n",
    "        outs2 = pipe(\n",
    "            texts[:need],\n",
    "            max_new_tokens=cfg.max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=T2,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "            eos_token_id=[tok.eos_token_id, eot_id],\n",
    "            batch_size=need,\n",
    "        )\n",
    "        rows = []\n",
    "        for m, o1, o2 in zip(metas[:need], outs1, outs2):\n",
    "            a, b = o1[0][\"generated_text\"].strip(), o2[0][\"generated_text\"].strip()\n",
    "            if random.random() < 0.5:\n",
    "                rows.append({\n",
    "                    \"pid\": m[\"pid\"],\n",
    "                    \"prompt\": m[\"prompt\"],\n",
    "                    \"a_text\": a,\n",
    "                    \"b_text\": b,\n",
    "                    \"a_meta\": {\"temperature\": T1, \"top_p\": 0.9},\n",
    "                    \"b_meta\": {\"temperature\": T2, \"top_p\": 0.9},\n",
    "                    \"order\": \"A=t1,B=t2\",\n",
    "                    \"model\": cfg.base_model,\n",
    "                })\n",
    "            else:\n",
    "                rows.append({\n",
    "                    \"pid\": m[\"pid\"],\n",
    "                    \"prompt\": m[\"prompt\"],\n",
    "                    \"a_text\": b,\n",
    "                    \"b_text\": a,\n",
    "                    \"a_meta\": {\"temperature\": T2, \"top_p\": 0.9},\n",
    "                    \"b_meta\": {\"temperature\": T1, \"top_p\": 0.9},\n",
    "                    \"order\": \"A=t2,B=t1\",\n",
    "                    \"model\": cfg.base_model,\n",
    "                })\n",
    "        append_jsonl_gz(GEN_JSONL, rows)\n",
    "        out_count += len(rows)\n",
    "        pbar.update(len(rows))\n",
    "finally:\n",
    "    pbar.close()\n",
    "    print(f\"[run {RUN_ID}] wrote {out_count} new pairs → {GEN_JSONL}\")\n",
    "    if os.path.exists(GEN_JSONL):\n",
    "        ds = load_dataset(\"json\", data_files=GEN_JSONL, split=\"train\")\n",
    "        print(\"Total pairs on disk:\", len(ds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
