{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe789dc-b3db-48b5-a749-b65cc0264135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup, get SFT adapter\n",
    "\n",
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "sft_adapter_path = \"workspace/output/cai_sft_stage1/adapters\"\n",
    "out_dir = \"dpo_adapter\"\n",
    "data_path = \"dpo_pairs.jsonl\"\n",
    "\n",
    "bnb = BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_quant_type=\"nf4\",bnb_4bit_compute_dtype=torch.bfloat16,bnb_4bit_use_double_quant=True)\n",
    "tok = AutoTokenizer.from_pretrained(base_model_id,use_fast=True)\n",
    "tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, sft_adapter_path)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "ds = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "\n",
    "def preprocess(batch):\n",
    "    return {\"prompt\": batch[\"prompt\"], \"chosen\": batch[\"chosen\"], \"rejected\": batch[\"rejected\"]}\n",
    "ds = ds.map(preprocess, remove_columns=[c for c in ds.column_names if c not in [\"prompt\",\"chosen\",\"rejected\"]])\n",
    "\n",
    "peft_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "cfg = DPOConfig(\n",
    "    output_dir=out_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=5e-6,\n",
    "    num_train_epochs=1,\n",
    "    max_length=2048,\n",
    "    max_prompt_length=1024,\n",
    "    beta=0.1,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=cfg,\n",
    "    tokenizer=tok,\n",
    "    train_dataset=ds\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983ade5-91eb-43f1-a11a-517aa8faa36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(out_dir)\n",
    "tok.save_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab71f5-da1c-4616-ae0a-c19187bd23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inference\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "dpo_adapter_path = \"dpo_adapter\"\n",
    "\n",
    "bnb = BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_quant_type=\"nf4\",bnb_4bit_compute_dtype=torch.bfloat16,bnb_4bit_use_double_quant=True)\n",
    "tok = AutoTokenizer.from_pretrained(base_model_id,use_fast=True)\n",
    "tok.pad_token = tok.eos_token\n",
    "\n",
    "m = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "m = PeftModel.from_pretrained(m, dpo_adapter_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baffc060-935e-48db-b12b-b8704edec81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check various adapter outputs\n",
    "prompt = \"A friend asked me to do their homework for them. What should I say?\"\n",
    "x = tok(prompt, return_tensors=\"pt\").to(m.device)\n",
    "with torch.inference_mode():\n",
    "    y = m.generate(**x, max_new_tokens=256, do_sample=True, temperature=0.7, top_p=0.9, eos_token_id=tok.eos_token_id)\n",
    "print(tok.decode(y[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
